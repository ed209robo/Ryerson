---
title: "CKME 136 Capstone Forest Fires"
output:
  word_document: default
  html_document: default
  df_print: paged
  rows.print: 25
---
## Load packages.
Link for "Initial Results and Code" on Github
https://github.com/ed209robo/Ryerson


```{r echo=TRUE}

#Make sure to use libraries below
library(dplyr)
#install.packages("ggplot2")
library(ggplot2)
#install.packages("ggpubr")
library(ggpubr)
#install.packages("tidyr")
library(tidyr)
#install.packages("scales")
library(scales)
#install.packages("wesanderson")
#library(wesanderson)
#install.packages("viridis")  # Install
library(viridis)           # Load
#theme_set(theme_pubclean())
library(ggrepel)
#install.packages("janitor")
library(janitor)
#install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
library(randomForest)
#install.packages("e1071")
library(e1071)
library(caret)
#install.packages("caretEnsemble")
library(caretEnsemble)
# install.packages("party")
library(party)
library(rpart)
#install.packages("formattable")
#detach(package:MASS, unload=TRUE)
#library(formattable)
#install.packages("DT")
#library(DT)
#install.packages("FSelector")
library(FSelector)
#??FSelector

```

2. Read the "fores fire .csv" file from the following website.

```{r echo=TRUE}
fires <-read.csv('CKME 136 Forest Fire Data.csv',header=T)
```

3. Have a look at the data set. View(train.data)

```{r echo=TRUE}
head(fires)
tail(fires)
str(fires) #properties and elements of (fires)

# Check data types of attributes
sapply(fires, class) 
sapply(fires, typeof)
```

4. Extract relevant columns.

```{r echo=TRUE}
new_fire <- fires[, c("Cause", "Jurisdiction", "Number", "Protection.zone", "Response.category", "Year")]
```

5. Check for missing values.

```{r echo=TRUE}
sum(is.na(new_fire$Cause) == TRUE) # 0 Missing values.
length(new_fire$Cause)

sum(is.na(new_fire$Jurisdiction) == TRUE) # 0 Missing values.
length(new_fire$Jurisdiction)

sum(is.na(new_fire$Number) == TRUE) # 8352 initial missing values for "Number" field. 
length(new_fire$Number)

sum(is.na(new_fire$Protection.zone) == TRUE) # 0 Missing values.
length(new_fire$Protection.zone)

sum(is.na(new_fire$Response.category) == TRUE) # 0 Missing values.
length(new_fire$Response.category)

sum(is.na(new_fire$Year) == TRUE) # 0 Missing values.
length(new_fire$Year)
```

6. Only "Number" has missing rows.  Remove all rows with missing values.

```{r echo=TRUE}

# Remove remaining records with missing values.
FireClean <- new_fire[complete.cases(new_fire),]

nrow(FireClean) #11519 rows remaining
```

7. Check attributes after missing rows have been removed.

```{r echo=TRUE}

#attach(FireClean)

head(FireClean)
tail(FireClean)
str(FireClean)
dim(FireClean) # 11519 rows, 6 columns

# Check data types of attributes
sapply(FireClean, class)

levels(FireClean$Cause)
levels(FireClean$Jurisdiction)
levels(FireClean$Protection.zone)
levels(FireClean$Response.category)
levels(FireClean$Year)
summary(FireClean) # Only the "number" attribute maybe usefull with the summary
```

8. Shorten "Jurisdiction" name

```{r echo=TRUE}

FireClean$Juris_Long <- FireClean$Jurisdiction # Duplicate Jurisdiction column

levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "British Columbia"] <- "BC"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Alberta"] <- "AB"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "National parks"] <- "NP"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Northwest Territories"] <- "NT"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Prince Edward Island"] <- "PE"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Yukon"] <- "YT"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "New Brunswick"] <- "NB"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Nova Scotia"] <- "NS"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Quebec"] <- "QC"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Manitoba"] <- "MB"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Newfoundland and Labrador"] <- "NL"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Ontario"] <- "ON"
levels(FireClean$Jurisdiction)[levels(FireClean$Jurisdiction) == "Saskatchewan"] <- "SK"
levels(FireClean$Jurisdiction)
```

9. create new column: Cause_Grouped  People vs Lightning

```{r echo=TRUE}
# Group Human causes together
# Lightning is a stand alone cause
FireClean <- FireClean %>%
  mutate(Cause_Grouped = case_when(
    Cause == "Lightning"  ~ "Lightning",
    TRUE                 ~ "People"
    )
  )
```

10. Create new column: Time1   Group Years

```{r echo=TRUE}
# Create new column "Time1" and group years into time periods
FireClean <- FireClean %>%
  mutate(Time1 = case_when(
    Year <= 1995                 ~ "Early 90s",
    Year >= 1996 & Year <= 2000  ~ "Late 90s",
    Year >= 2001 & Year <= 2005  ~ "Early 10s",
    Year >= 2006 & Year <= 2010  ~ "Late 10s",
    Year >= 2011 & Year <= 2015  ~ "Early 20s",
    Year >= 2016                 ~ "Late 20s"
    )
  )
```

11. Create new column: Time2   Group Years

```{r echo=TRUE}
# Create new column "Time2" and group years into time periods
FireClean <- FireClean %>%
  mutate(Time2 = case_when(
    Year >= 1990 & Year <= 1999  ~ "1990s",
    Year >= 2000 & Year <= 2009  ~ "2000s",
    Year >= 2010 & Year <= 2018  ~ "2010s"
    )
  )
```

12. Group Provinces into regions

```{r echo=TRUE}
# Create new column "Region" and group provinces into regions
FireClean <- FireClean %>% 
  mutate(Region = case_when(
    Jurisdiction %in% c("AB", "MB", "SK")           ~ "Prairie Region",
    Jurisdiction %in% c("BC")                       ~ "Pacific Region",
    Jurisdiction %in% c("NP")                       ~ "National Parks",
    Jurisdiction %in% c("NB", "NL", "NS", "PE")     ~ "Atlantic Region",
    Jurisdiction %in% c("ON", "QC")                 ~ "Central Region",
    Jurisdiction %in% c("YT", "NT")                 ~ "North Region"
      )
  )
```

13. Check structure of attributes again.

```{r echo=TRUE}
#attach(FireClean)

head(FireClean)
tail(FireClean)
str(FireClean)
dim(FireClean) # 11519 rows, 6 columns

# Check data types of attributes
sapply(FireClean, class)

levels(FireClean$Cause)
levels(FireClean$Jurisdiction)
levels(FireClean$Protection.zone)
levels(FireClean$Response.category)
levels(FireClean$Year)
summary(FireClean) # Only the "number" attribute maybe usefull with the summary
summary(FireClean$Number) 
summary(FireClean$Year) 
```

14. Feature selection using FSelector
    Use information gain to find best attributes.

```{r echo=TRUE, rows.print=30, cols.print=30}

data <- tbl_df(FireClean)

head(data)

weights <- information.gain(Number~., data)
print(weights)

subset <- cutoff.k(weights, 12)

f <- as.simple.formula(subset, "Number")
print(f)

```

15. Check structure of data

```{r echo=TRUE, rows.print=30, cols.print=30}
head(data)
tail(data)
str(data)
dim(data) # 11519 rows, 6 columns

# Check data types of attributes
sapply(data, class)

levels(data$Cause)
levels(data$Jurisdiction)
levels(data$Protection.zone)
levels(data$Response.category)
levels(data$Year)
summary(data) # Only the "number" attribute maybe usefull with the summary
summary(data$Number) 
summary(data$Year) 
```

16. Visulization, Barplot, Boxplot, Exploratory Data Analysis, Outliers

```{r echo=TRUE, rows.print=30, cols.print=30}
########################################################################################################################################
#11 STACKED BAR CHART (STACKED BY CAUSE) FOR FOREST FIRES IN BC 1990 - 2018
pivot3 <- data %>%
dplyr::select(Year, Number, Cause_Grouped)
head(pivot3) 

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Cause_Grouped, Number, Year, Jurisdiction) %>% 
  filter(Jurisdiction == "BC") %>% 
  group_by(Year, Cause_Grouped) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 

# Matrix with Cause-Grouped (Lightning & Human), Total number of fires for each year.
pivot3 %>% 
  spread(Year, sum_Number)

############
#SUMMING COLUMNS AND ROWS
pivot4 <- pivot3 %>% 
  spread(Year, sum_Number)

pivot4 # Matrix with Cause-Grouped (Lightning & Human), Total number of fires for each year.

pivot5 <- pivot4 %>%
  adorn_totals("row") %>% 
  adorn_totals("col") 

pivot5 # Matrix WITH TOTALS for Cause-Grouped (Lightning & Human), Total number of fires for each year.
##################################################
# Function to make number scale easier to read on x-axis
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }

p <- ggplot(data = pivot3, aes(x = Year,
                               y = sum_Number,
                               color = Cause_Grouped)) +
  geom_bar(stat="identity", width = 0.7, fill="white") +
  # geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
  #                 vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  scale_x_continuous(breaks=1990:2018)+
  #stat_summary(fun.y = sum, aes(label = ..y.., group = Year), geom = "text", vjust= -1.5, show_guide = F)+
  xlab("Cause") +
  ylab("Causes of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 5000))+
  labs(title = "Causes of Forest Fires in BC (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 0 ,size = 9, angle = 90))
########################################################################################################################################
#10B Matrix WITH TOTALS for Regions and total number of fires for each cause (Lightning & Human) in the region.
pivot3 <- data %>%
dplyr::select(Region, Number, Cause_Grouped)
head(pivot3)

pivot3 <- data %>% #Groups Cause and Regions together, sums Number
  dplyr::select(Region, Number, Cause_Grouped) %>% 
  group_by(Cause_Grouped, Region) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE))
  
# Matrix WITH TOTALS for Regions and total number of fires for each cause (Lightning & Human) in the region.
pivot3 %>% 
  spread(Cause_Grouped, sum_Number)

############
#SUMMING COLUMNS AND ROWS
pivot4 <- pivot3 %>% 
  spread(Cause_Grouped, sum_Number)

pivot4

pivot5 <- pivot4 %>%
  adorn_totals("row") %>% 
  adorn_totals("col") 

pivot5
##################################################
#CORRELATION USE FOR pivot3 ONLY
CorDataFrame <- pivot3 %>% 
  spread(Cause_Grouped, sum_Number)

CorDataFrame
sapply(CorDataFrame, is.numeric) # Which columns are numeric?

my_num_data <- CorDataFrame[, sapply(CorDataFrame, is.numeric)] # Subset numeric columns
my_num_data


plot(my_num_data) # Works
cor(my_num_data)
##################################################
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }

p <- ggplot(data = pivot3,
       aes(x = Region, y = sum_Number, color=Cause_Grouped)) +
  geom_bar(position = "dodge", stat="identity", width = 0.7, fill="white") +
  geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
                  vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  xlab("Cause") +
  ylab("Causes of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 40000))+
  labs(title = "Causes of Forest Fires (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 1,size = 9, angle = 45))
########################################################################################################################################
#10 Matrix WITH TOTALS for Regions and total number of fires for each cause (Lightning & Human) in the region.
pivot3 <- data %>%
dplyr::select(Region, Number, Cause_Grouped)
head(pivot3)

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Region, Number, Cause_Grouped) %>% 
  group_by(Cause_Grouped, Region) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 

pivot3 %>% 
  spread(Cause_Grouped, sum_Number)
##################################################
#CORRELATION
CorDataFrame <- pivot3 %>% 
  spread(Cause_Grouped, sum_Number)

CorDataFrame
sapply(CorDataFrame, is.numeric) # Which columns are numeric?

my_num_data <- CorDataFrame[, sapply(CorDataFrame, is.numeric)] # Subset numeric columns
my_num_data

plot(my_num_data) # Works
cor(my_num_data)
##################################################
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }

p <- ggplot(data = pivot3,
       aes(x = Region, y = sum_Number, color=Cause_Grouped)) +
  geom_bar(position = "dodge", stat="identity", width = 0.7, fill="white") +
  geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
                  vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  xlab("Cause") +
  ylab("Causes of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 40000))+
  labs(title = "Causes of Forest Fires (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 1,size = 9, angle = 45))
########################################################################################################################################
#9 Time Series chart for number of fires in each province (1990-2018)
pivot3 <- data %>%
dplyr::select(Year, Number, Jurisdiction)
head(pivot3)

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Jurisdiction, Number, Year) %>% 
  group_by(Year, Jurisdiction) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 

pivot3 %>% 
  spread(Year, sum_Number)

#Produces matrix with zero
pivot4 <- pivot3 %>% 
  spread(Year, sum_Number)

pivot4

#Remove NAs from matrix, replace with zero
pivot4[is.na(pivot4)] <- 0
#Matrix now has NAs removed, now replaced with zero
pivot4

############
#SUMMING COLUMNS AND ROWS

pivot5 <- pivot4 %>%
  adorn_totals("row") %>% 
  adorn_totals("col") 

pivot5
##################################################
# Test 4 - Time Series chart for number of fires in each province (1990-2018) with thicker lines
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }


p <- ggplot(data = pivot3, aes(x = Year,
                               y = sum_Number,
                               color = Jurisdiction)
            ) +
  geom_line(stat="identity", width = 0.7, fill="white", size = 2) +
  # geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
  #                 vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7",
                                "#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442")) +
  scale_x_continuous(breaks=1990:2018)+
  #stat_summary(fun.y = sum, aes(label = ..y.., group = Year), geom = "text", vjust= -1.5, show_guide = F)+
  xlab("Year") +
  ylab("Number of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 4500))+
  labs(title = "Number of Forest Fires for each Province (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 0 ,size = 9, angle = 90))
##################################################
# Test 3 - Time Series chart for number of fires in each province (1990-2018) with thicker lines
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }


p <- ggplot(data = pivot3, aes(x = Year,
                               y = sum_Number,
                               color = Jurisdiction)
            ) +
  geom_line(stat="identity", width = 0.7, fill="white", size = 2) +
  # geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
  #                 vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  scale_x_continuous(breaks=1990:2018)+
  #stat_summary(fun.y = sum, aes(label = ..y.., group = Year), geom = "text", vjust= -1.5, show_guide = F)+
  xlab("Year") +
  ylab("Number of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 4500))+
  labs(title = "Number of Forest Fires for each Province (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 0 ,size = 9, angle = 90))
##################################################
# Test 2 - Time Series chart for number of fires in each province (1990-2018) with thin lines
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }


p <- ggplot(data = pivot3, aes(x = Year,
                               y = sum_Number,
                               color = Jurisdiction)
            ) +
  geom_line(stat="identity", width = 0.7, fill="white") +
  # geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
  #                 vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  scale_x_continuous(breaks=1990:2018)+
  #stat_summary(fun.y = sum, aes(label = ..y.., group = Year), geom = "text", vjust= -1.5, show_guide = F)+
  xlab("Year") +
  ylab("Number of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 4500))+
  labs(title = "Number of Forest Fires for each Province (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 0 ,size = 9, angle = 90))
########################################################################################################################################
#8 - Stacked bar (cause of fire, Lightning vs Human) chart for all provinces by (1990-2018)
pivot3 <- data %>%
dplyr::select(Year, Number, Cause_Grouped)
head(pivot3)

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Cause_Grouped, Number, Year) %>% 
  group_by(Year, Cause_Grouped) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 

pivot3 %>% 
  spread(Year, sum_Number)
##################################################
# Test 8 Stacked bar (cause of fire, Lightning vs Human) chart for all provinces by (1990-2018)
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }

p <- ggplot(data = pivot3, aes(x = Year,
                               y = sum_Number,
                               color = Cause_Grouped)
            ) +
  geom_bar(stat="identity", width = 0.7, fill="white") +
  # geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
  #                 vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  scale_x_continuous(breaks=1990:2018)+
  #stat_summary(fun.y = sum, aes(label = ..y.., group = Year), geom = "text", vjust= -1.5, show_guide = F)+
  xlab("Cause") +
  ylab("Causes of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 12000))+
  labs(title = "Causes of Forest Fires (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 0 ,size = 9, angle = 90))
########################################################################################################################################
#7 Boxplot outlier detection
pivot3 <- data %>%
dplyr::select(Jurisdiction, Number, Year)
head(pivot3)

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Jurisdiction, Number, Year) %>% 
  group_by(Year, Jurisdiction) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 
  
#Produces matrix with zero
pivot4 <- pivot3 %>% 
  spread(Year, sum_Number)

pivot4

#Remove NAs from matrix, replace with zero
pivot4[is.na(pivot4)] <- 0
#Matrix now has NAs removed, now replaced with zero
pivot4

############
#SUMMING COLUMNS AND ROWS

pivot5 <- pivot4 %>%
  adorn_totals("row") %>% 
  adorn_totals("col") 

pivot5
##################################################
boxplot(sum_Number~Year,
  data=pivot3,
  main="Distribution of number of fires for all Provinces per year \n(1990 - 2018)",
  xlab="Year",
  ylab="Number of Fires",
  col=c("blue", "indianred1","red","orange","purple","green",
    "pink","brown", "mediumpurple2","olivedrab1"),
  border="black"
)

# Outlier detection
OutVals = boxplot(pivot3)$out
OutVals

# Find Outlier index position
#which(pivot3 )
########################################################################################################################################
#6 Boxplot - outlier detection
pivot3 <- data %>%
dplyr::select(Jurisdiction, Number, Year)
head(pivot3)

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Jurisdiction, Number, Year) %>% 
  group_by(Year, Jurisdiction) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 

pivot3 %>% 
  spread(Jurisdiction, sum_Number)

#Produces matrix with zero
pivot4 <- pivot3 %>% 
  spread(Jurisdiction, sum_Number)

pivot4

#Remove NAs from matrix, replace with zero
pivot4[is.na(pivot4)] <- 0
#Matrix now has NAs removed, now replaced with zero
pivot4
############
#SUMMING COLUMNS AND ROWS

pivot5 <- pivot4 %>%
  adorn_totals("row") %>% 
  adorn_totals("col") 

pivot5
##################################################
#CORRELATION
CorDataFrame <- pivot3 %>% 
  spread(Jurisdiction, sum_Number)

CorDataFrame
sapply(CorDataFrame, is.numeric) # Which columns are numeric?

my_num_data <- CorDataFrame[, sapply(CorDataFrame, is.numeric)] # Subset numeric columns
my_num_data

plot(my_num_data) # Works
cor(my_num_data)
##################################################
# Test 1
boxplot(sum_Number~Jurisdiction,
  data=pivot3,
  main="Distribution of number of fires in each Province per year \n(1990 - 2018)",
  xlab="Provinces",
  ylab="Number of Fires",
  col=c("blue", "indianred1","red","orange","purple","green",
    "pink","brown", "mediumpurple2","olivedrab1"),
  border="black"
)

# Outlier detection
OutVals = boxplot(pivot3)$out
OutVals
########################################################################################################################################
#5 Boxplot - outlier detection
pivot3 <- data %>%
dplyr::select(Jurisdiction, Number, Cause_Grouped)
head(pivot3)

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Jurisdiction, Number, Cause_Grouped) %>% 
  group_by(Cause_Grouped, Jurisdiction) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 
  
pivot3 %>% 
  spread(Cause_Grouped, sum_Number)
###############################################
# Test 2
boxplot(sum_Number~Cause_Grouped,
  data=pivot3,
  main="Distribution of number of fires in each Province for each cause \n(1990 - 2018)",
  xlab="Cause",
  ylab="Number of Fires",
  col=c("blue", "indianred1","red","orange","purple","green",
    "pink","brown", "mediumpurple2","olivedrab1"),
  border="black"
)

# Outlier detection
#OutVals = boxplot(pivot3)$out
#OutVals
########################################################################################################################################
#4
pivot3 <- data %>%
dplyr::select(Jurisdiction, Number, Cause_Grouped)
head(pivot3)

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Jurisdiction, Number, Cause_Grouped) %>% 
  group_by(Cause_Grouped, Jurisdiction) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 
  
pivot3 %>% 
  spread(Cause_Grouped, sum_Number)
##################################################
# Test 5
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }

p <- ggplot(data = pivot3, aes(x = Jurisdiction,
                               y = sum_Number,
                               color = Cause_Grouped)
            ) +
  geom_bar(stat="identity", width = 0.7, fill="white") +
  # geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
  #                 vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  stat_summary(fun.y = sum, aes(label = ..y.., group = Jurisdiction), geom = "text", vjust= -1.5, show_guide = F)+
  xlab("Cause") +
  ylab("Causes of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 60000))+
  labs(title = "Causes of Forest Fires (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 1,size = 9, angle = 0))
########################################################################################################################################
#3
pivot3 <- data %>%
dplyr::select(Jurisdiction, Number, Cause_Grouped)
head(pivot3)

# attach(pivot3)
# detach(pivot3)

pivot3 <- data %>% #Groups Cause together and sums Number
  dplyr::select(Jurisdiction, Number, Cause_Grouped) %>% 
  group_by(Cause_Grouped, Jurisdiction) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE)) 
  
pivot3 %>% 
  spread(Cause_Grouped, sum_Number)
##################################################
#CORRELATION
CorDataFrame <- pivot3 %>% 
  spread(Cause_Grouped, sum_Number)

CorDataFrame
sapply(CorDataFrame, is.numeric) # Which columns are numeric?

my_num_data <- CorDataFrame[, sapply(CorDataFrame, is.numeric)] # Subset numeric columns
my_num_data

plot(my_num_data) # Works
cor(my_num_data)
##################################################
# Test 11
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }


p <- ggplot(data = pivot3,
       aes(x = Jurisdiction, y = sum_Number, color=Cause_Grouped)) +
  geom_bar(position = "dodge", stat="identity", width = 0.7, fill="white") +
  geom_text_repel(aes(label=sum_Number), show_guide = F, position=position_dodge(width=0.4),
                  vjust= -2.4, hjust = 0.4, size = 3.8, angle = 0)+
  xlab("Cause") +
  ylab("Causes of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 40000))+
  labs(title = "Causes of Forest Fires (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 1,size = 9, angle = 0))
########################################################################################################################################
#2
#Below works with percentage totals
GCause_HvsL <- data %>% #Groups Cause together and sums Number
  group_by(Cause_Grouped) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE)) %>% 
  mutate(rel.freq = paste0(round(100 * sum_Number/sum(sum_Number),0),"%"))

GCause_HvsL
##################################################
# Boxplot GCause 11, testing
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }

p <- ggplot(data = GCause_HvsL,
       aes(x = Cause_Grouped, y = sum_Number, color=Cause_Grouped)) +
  geom_bar(stat="identity", width = 0.3, fill="white") +
  geom_text(aes(label=sum_Number), position=position_dodge(width=0.9), vjust=-0.15)+
  xlab("Cause") +
  ylab("Causes of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 125000))+
  labs(title = "Causes of Forest Fires (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 1,size = 9, angle = 45), legend.position = "none")
########################################################################################################################################
#1
#Below works with percentage totals
GCause <- data %>% #Groups Cause together and sums Number
  group_by(Cause) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE)) %>% 
  mutate(rel.freq = paste0(round(100 * sum_Number/sum(sum_Number),0),"%"))

GCause
##################################################
ks <- function (x) { number_format(accuracy = 1,
                                   scale = 1/1000,
                                   suffix = "k",
                                   big.mark = ",")(x) }

p <- ggplot(data = GCause,
       aes(x = Cause, y = sum_Number, color=Cause)) +
  geom_bar(stat="identity", fill="white") +
  geom_text(aes(label=sum_Number), position=position_dodge(width=0.9), vjust=-0.15)+
  xlab("Cause") +
  ylab("Causes of Forest Fires (1990 - 2018)") +
  scale_y_continuous(name="Sum of Forest Fires", labels = ks)+
  coord_cartesian(ylim = c(0, 105000))+
  labs(title = "Causes of Forest Fires (1990 - 2018)")

p
p + theme(
  axis.text.x = element_text(face = "bold", color = "#993333", hjust = 1,size = 9, angle = 45), legend.position = "none")
########################################################################################################################################
#Sample code for various statistics
data %>% #Groups Cause, Year together and sums Number
  group_by(Cause, Year) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))    

#Below code works
data %>% #Groups Cause, Year together and sums Number
  group_by(Cause, Jurisdiction, Year) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))    

#Below code works
data %>% #Groups Cause, Year together and sums Number
  group_by(Jurisdiction, Year) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))    

#Below code works
data %>% #Groups Cause, Year together and sums Number
  group_by(Year, Jurisdiction) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))    

#Below code works
data %>% #Groups Cause, Year together and sums Number
  group_by(Jurisdiction) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))    

#Below code works
data %>% #Groups Cause, Year together and sums Number
  group_by(Jurisdiction) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))    

#######################################################
#Below code works
data %>% #Groups Cause, Year, Jurisdiction and sums Number
  group_by(Cause, Year, Jurisdiction) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE)) %>% 
  filter(Jurisdiction == "BC") %>%
  filter(Cause == "Lightning")

#Below code works
data %>% #Groups Cause together and sums Number
  group_by(Cause, Year, Jurisdiction) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE)) %>% 
  filter(Jurisdiction == "AB") %>%
  filter(Cause == "Lightning")

#Below works great, show max number of fires and year for Alberta
data %>% #Groups Cause together and sums Number
  group_by(Jurisdiction, Cause, Year) %>% 
  summarize(max_Number = max(Number, na.rm = TRUE)) %>% 
  top_n(n=1) %>% 
  filter(Jurisdiction == "AB") %>%
  filter(Cause == "Lightning") 

#Below works great, Top 10 years for Lightning in Alberta
data %>% #Groups Cause together and sums Number
  group_by(Jurisdiction, Cause, Year) %>% 
  summarize(max_Number = max(Number, na.rm = TRUE)) %>% 
  top_n(n=10) %>% 
  filter(Jurisdiction == "AB") %>%
  filter(Cause == "Lightning") %>% 
  arrange(desc(max_Number))

#Below works great, Top 10 years for Lightning in Alberta
data %>% #Groups Cause together and sums Number
  group_by(Jurisdiction, Cause, Year) %>% 
  summarize(max_Number = max(Number, na.rm = TRUE)) %>% 
  top_n(n=10) %>% 
  filter(Jurisdiction == "AB") %>%
  filter(Cause == "Lightning")
  
#Below works great, Top 10 years for Lightning in Alberta
Alberta <- data %>% #Groups Cause together and sums Number
  group_by(Jurisdiction, Cause, Year) %>% 
  summarize(max_Number = max(Number, na.rm = TRUE)) %>% 
  top_n(n=10) %>% 
  filter(Jurisdiction == "AB") %>%
  filter(Cause == "Lightning")

Alberta

hist(Alberta$Year)
hist(Alberta$max_Number)

#Below works
data %>% #Groups Cause together and sums Number
  group_by(Jurisdiction, Cause, Year) %>% 
  summarize(max_Number = max(Number, na.rm = TRUE)) %>% 
  filter(Jurisdiction == "AB") %>%
  filter(Cause == "Lightning") %>% 
  filter(Year > "2003")
  
#Below works, Top 10 after 2003
data %>% #Groups Cause together and sums Number
  group_by(Jurisdiction, Cause, Year) %>% 
  summarize(max_Number = max(Number, na.rm = TRUE)) %>% 
  filter(Jurisdiction == "AB") %>%
  filter(Cause == "Lightning") %>% 
  filter(Year > "2003") %>% 
  arrange(desc(max_Number))

data %>% 
  group_by(Jurisdiction, Cause, Year) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))

#Below works, Top 10 after 2003
data %>% #Groups Cause together and sums Number
  group_by(Jurisdiction, Cause, Year) %>% 
  summarize(max_Number = max(Number, na.rm = TRUE)) %>% 
  filter(Jurisdiction == "AB") %>%
  filter(Cause == "Lightning") %>% 
  filter(Year > "2003") %>% 
  arrange(desc(max_Number))

data %>% #Groups Cause together and sums Number.  Also provides avg for Number of fires.
  group_by(Cause) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE),
    count = n(),
    average_fire = mean(Number, na.rm = TRUE))

data %>% #Groups Cause together and sums Number.  Also provides avg for Number of fires.
  group_by(Cause) %>% 
  summarize(
    sum_Number = sum(Number, na.rm = TRUE),
    count = n(),
    average_fire = mean(Number, na.rm = TRUE),
    total = sum(sum_Number)
    )

data %>% #Groups Year and sums Number
  group_by(Year) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))

data %>% #Groups Jurisdiction and sums Number
  group_by(Jurisdiction) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))

data %>% 
  group_by(Year) %>%
  group_by(Jurisdiction) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))

data %>% 
  group_by(Jurisdiction) %>%
  group_by(Year) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))

data %>% 
  group_by(Jurisdiction) %>%
  group_by(Year) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))

data %>% 
  group_by(Jurisdiction) %>%
  group_by(Year) %>% 
  summarize(sum_Number = sum(Number, na.rm = TRUE))

data %>% 
  group_by(Jurisdiction, Cause, Year) %>%
  summarize(sum_Number = sum(Number, na.rm = TRUE))

data$Fire_Cause_Human = as.character(data$Cause)

data$Fire_Cause_Human[ data$Fire_Cause_Human != "Lightning" & data$Fire_Cause_Human != "Unspecified" ] = "Human"
# There are three types of fire causes, Lightning, Human and Unspecified.  

unique(data$Fire_Cause_Human)
#[1] "Human"       "Lightning"   "Unspecified"

names(data)
#  [1] "Cause"             "Jurisdiction"      "Number"            "Protection.zone"   "Response.category" "Year"              "Juris_Long"        "Cause_Grouped"     "Time1"             "Time2"            
# [11] "Region"            "Fire_Cause_Human" 

unique(data$Cause_Grouped)
#[1] "People"    "Lightning"

unique(data$Year)


```

## Outlier detection

```{r echo=TRUE, rows.print=30, cols.print=30}

# Treating the outliers with mean/median imputation
# We can handle outliers with mean or median imputation by replacing the observations lower than the 5th percentile with mean 
# and those higher than 95th percentile with median. 
# We can use the same statistics, mean or median, to impute outliers in both directions:

# Outlier detection
outlier_values <- boxplot.stats(data$Number)$out
boxplot(data$Number, main="Number", boxwex=0.1)
plot(outlier_values)


impute_outliers <- function(x,removeNA = TRUE){
    quantiles <- quantile( x, c(.05, .95 ),na.rm = removeNA )
    x[ x < quantiles[1] ] <- mean(x,na.rm = removeNA )
    x[ x > quantiles[2] ] <- median(x,na.rm = removeNA )
    x
}

imputed_data <- impute_outliers(data$Number)

par(mfrow = c(1, 2))

boxplot(data$Number, main="Forest Fire Numbers having Outliers", boxwex=0.3)

boxplot(imputed_data, main="Forest Fire Number with imputed data", boxwex=0.3)

########################################################################################################################################
# Handling extreme values with capping
# To handle extreme values that lie outside the 1.5 * IQR(Inter Quartile Range) limits, 
# we could cap them by replacing those observations that lie below the lower limit, 
# with the value of 5th percentile and those that lie above the upper limit, 
# with the value of 95th percentile, 

replace_outliers <- function(x, removeNA = TRUE) {
     Number <- x
     qnt <- quantile(Number, probs=c(.25, .75), na.rm = removeNA)
     caps <- quantile(Number, probs=c(.05, .95), na.rm = removeNA)
     H <- 1.5 * IQR(Number, na.rm = removeNA)
     Number[Number < (qnt[1] - H)] <- caps[1]
     Number[Number > (qnt[2] + H)] <- caps[2]
     Number
 }

capped_Number <- replace_outliers(data$Number)

par(mfrow = c(1, 2))

boxplot(data$Number, main="Forest Fire Numbers with Outliers", boxwex=0.1)

boxplot(capped_Number, main="Forest Fire Numbers without Outliers", boxwex=0.1)

```

## 17. Prepare packages for models

```{r echo=TRUE, rows.print=30, cols.print=30}

library(caret)
library(lars)
library(elasticnet)

```

## 18. 

## Split the data into training and test set
## Set the seed to make your partition reproducible
## We want to make sure that the training set and the test set do not have any common data points.
## R built in function "Sample"" randomly selects samples

```{r echo=TRUE, rows.print=30, cols.print=30}

set.seed(123) 
train_index <- sample(1:nrow(data), 0.7 * nrow(data))
train.set <- data[train_index,]
test.set  <- data[-train_index,]

```

## 19. 
## Set up model
## Set up K-fold cross-validation
## Defining the training controls for multiple models


```{r echo=TRUE, rows.print=30, cols.print=30}

train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)

```

## 20. 
## Build the models with start time and end time for each model

```{r echo=TRUE, rows.print=30, cols.print=30}

#############################################################################################################################################
# Model 1: lm model
# Start the clock
start.time <- Sys.time()

set.seed(123) 
lm_model <- train(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set, method = "lm",
               trControl = train.control)

# Stop the clock
end.time <- Sys.time()

lm_model_time.taken <- end.time - start.time
#lm_model_time.taken

#############################################################################################################################################
# Model 2: glm model
# Start the clock
start.time <- Sys.time()

set.seed(123) 
glm_model <- train(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set, method = "glm",
               trControl = train.control)

# Stop the clock
end.time <- Sys.time()

glm_model_time.taken <- end.time - start.time
#glm_model_time.taken

#############################################################################################################################################
# Model 3: lasso model
# Start the clock
start.time <- Sys.time()

set.seed(123)
lasso_Mod <- train(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set, method = "lasso",
               trControl = train.control)

# Stop the clock
end.time <- Sys.time()

lasso_Mod_time.taken <- end.time - start.time
#lasso_Mod_time.taken
#############################################################################################################################################
# Model 4: knn model
# Start the clock
start.time <- Sys.time()

set.seed(123)
knn_model <- train(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set, method = "knn",
               trControl = train.control)

# Stop the clock
end.time <- Sys.time()

knn_model_time.taken <- end.time - start.time
#knn_model_time.taken
#############################################################################################################################################
# Model 5: leapForward model
# Start the clock
start.time <- Sys.time()

set.seed(123)
LF_model <- train(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set, method = "leapForward",
               trControl = train.control)

# Stop the clock
end.time <- Sys.time()

LF_model_time.taken <- end.time - start.time
#LF_model_time.taken
#############################################################################################################################################
# Model 6: leapBackward model
# Start the clock
start.time <- Sys.time()

set.seed(123)
LB_model <- train(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set, method = "leapBackward",
               trControl = train.control)

# Stop the clock
end.time <- Sys.time()

LB_model_time.taken <- end.time - start.time
#LB_model_time.taken
#############################################################################################################################################
# Model 7: lmStepAIC model
# Start the clock
start.time <- Sys.time()

set.seed(123)
lmStepAIC_Mod <- train(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set, method = "lmStepAIC",
               trControl = train.control)

# Stop the clock
end.time <- Sys.time()

lmStepAIC_Mod_time.taken <- end.time - start.time
#lmStepAIC_Mod_time.taken
#############################################################################################################################################


```

## 21. 
## View the total amount of time taken to run each model.

```{r echo=TRUE, rows.print=30, cols.print=30}

Model_Time <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         Time_train=c(lm_model_time.taken, glm_model_time.taken, lasso_Mod_time.taken, knn_model_time.taken, LF_model_time.taken, LB_model_time.taken, lmStepAIC_Mod_time.taken))
Model_Time

```

## 22. 
## View summaries of the models.

```{r echo=TRUE, rows.print=30, cols.print=30}

############################################
# Model 1
# Summarize the results for lm_model
print(lm_model)
summary(lm_model)
lm_model$finalModel
lm_model$modelType
############################################
# Model 2
# Summarize the results for glm model
print(glm_model)
summary(glm_model)
glm_model$finalModel
glm_model$modelType
############################################
# Model 3
# Summarize the results
print(lasso_Mod)
summary(lasso_Mod)
lasso_Mod$finalModel
lasso_Mod$modelType
############################################
# Model 4
# Summarize the results for knn model
print(knn_model)
summary(knn_model)
knn_model$finalModel
knn_model$modelType
############################################
# Model 5
# Summarize the results for LF model
print(LF_model)
summary(LF_model)
LF_model$finalModel
LF_model$modelType
############################################
# Model 6
# Summarize the results for LB Modle
print(LB_model)
summary(LB_model)
LB_model$finalModel
LB_model$modelType
############################################
# Model 7
# Summarize the results for lmStepAIC
print(lmStepAIC_Mod)
summary(lmStepAIC_Mod)
lmStepAIC_Mod$finalModel
lmStepAIC_Mod$modelType
############################################

```

## 23.
## Evaluation of techniques
## View the results of the models

```{r echo=TRUE, rows.print=30, cols.print=30}

# Evaluation of techniques
results <- resamples(list(lm=lm_model, glm=glm_model,lasso=lasso_Mod, knn=knn_model, leapForward=LF_model,leapBackward=LB_model, lmStepAIC=lmStepAIC_Mod))
summary(results)

# Compare results with boxplots 
bwplot(results)
# Compare results with dot plots 
dotplot(results)

```

## 24.
## Find the best results for each model.

```{r echo=TRUE, rows.print=30, cols.print=30}


get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

###################################################################################
lm_best <- data.frame(get_best_result(lm_model) %>%
    dplyr::select(2:4))
lm_model$finalModel
###################################################################################
glm_best <- data.frame(get_best_result(glm_model) %>% 
                         dplyr::select(2:4))
glm_model$finalModel
###################################################################################
lasso_best <- data.frame(get_best_result(lasso_Mod) %>% 
                         dplyr::select(2:4))
lasso_Mod$finalModel
###################################################################################
knn_best <- data.frame(get_best_result(knn_model) %>% 
                         dplyr::select(2:4))
knn_model$finalModel
###################################################################################
LF_best <- data.frame(get_best_result(LF_model) %>% 
                         dplyr::select(2:4))
LF_model$finalModel
###################################################################################
LB_best <- data.frame(get_best_result(LB_model) %>% 
                         dplyr::select(2:4))
LB_model$finalModel
###################################################################################
lmStep_best <- data.frame(get_best_result(lmStepAIC_Mod) %>% 
                         dplyr::select(2:4))
lmStepAIC_Mod$finalModel
###################################################################################
total <- rbind(lm_best, glm_best, lasso_best, knn_best, LF_best, LB_best, lmStep_best)
total_best_train <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         total,
                         Model_Time)%>% 
                         dplyr::select(-5)
total_best_train
```

## 25. Predict on test set

```{r echo=TRUE, rows.print=30, cols.print=30}

#############################################################################################################################################
# Model 1: lm model
start.time <- Sys.time()

set.seed(123) 
pred.lm = predict(lm_model, newdata = test.set)
output <- cbind(test.set, pred.lm)
head(output)
plot(pred.lm)

# Stop the clock
end.time <- Sys.time()

pre.lm_model_time.taken <- end.time - start.time
#############################################################################################################################################
# Model 2: glm model
# Start the clock
start.time <- Sys.time()

set.seed(123) 
pred.glm_model = predict(glm_model, newdata = test.set)
output <- cbind(test.set, pred.glm_model)
head(output)
plot(pred.glm_model)

# Stop the clock
end.time <- Sys.time()

pre.glm_model_time.taken <- end.time - start.time
#glm_model_time.taken

#############################################################################################################################################
# Model 3: lasso model
# Start the clock
start.time <- Sys.time()

set.seed(123) 
pred.lasso_Mod = predict(lasso_Mod, newdata = test.set)
output <- cbind(test.set, pred.lasso_Mod)
head(output)
plot(pred.lasso_Mod)

# Stop the clock
end.time <- Sys.time()

pre.lasso_Mod_time.taken <- end.time - start.time
#lasso_Mod_time.taken
#############################################################################################################################################
# Model 4: knn model
# Start the clock
start.time <- Sys.time()

set.seed(123) 
pred.knn_model = predict(knn_model, newdata = test.set)
output <- cbind(test.set, pred.knn_model)
head(output)
plot(pred.knn_model)

# Stop the clock
end.time <- Sys.time()

pre.knn_model_time.taken <- end.time - start.time
#knn_model_time.taken
#############################################################################################################################################
# Model 5: leapForward model
# Start the clock
start.time <- Sys.time()

set.seed(123) 
pred.LF_model = predict(LF_model, newdata = test.set)
output <- cbind(test.set, pred.LF_model)
head(output)
plot(pred.LF_model)

# Stop the clock
end.time <- Sys.time()

pre.LF_model_time.taken <- end.time - start.time
#LF_model_time.taken
#############################################################################################################################################
# Model 6: leapBackward model
# Start the clock
start.time <- Sys.time()

set.seed(123) 
pred.LB_model = predict(LB_model, newdata = test.set)
output <- cbind(test.set, pred.LB_model)
head(output)
plot(pred.LB_model)

# Stop the clock
end.time <- Sys.time()

pre.LB_model_time.taken <- end.time - start.time
#LB_model_time.taken
#############################################################################################################################################
# Model 7: lmStepAIC model
# Start the clock
start.time <- Sys.time()

set.seed(123) 
pred.lmStepAIC_Mod = predict(lmStepAIC_Mod, newdata = test.set)
output <- cbind(test.set, pred.lmStepAIC_Mod)
head(output)
plot(pred.lmStepAIC_Mod)

# Stop the clock
end.time <- Sys.time()

pre.lmStepAIC_Mod_time.taken <- end.time - start.time
#lmStepAIC_Mod_time.taken
#############################################################################################################################################


```

## 26. Time taken for prediction

```{r echo=TRUE, rows.print=30, cols.print=30}

Model_TimePred <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         Predict_Time=c(pre.lm_model_time.taken, pre.glm_model_time.taken, pre.lasso_Mod_time.taken, pre.knn_model_time.taken, pre.LF_model_time.taken, pre.LB_model_time.taken, pre.lmStepAIC_Mod_time.taken))
Model_TimePred

```

## 27. Compare correlation between actual and predicted 

```{r echo=TRUE, rows.print=30, cols.print=30}

#############################################################################################################################################
# Formula to calculate correlation 
corr_lm_model <- round(cor(test.set$Number, predict(lm_model, test.set), method = c("pearson", "kendall", "spearman")),2)
corr_glm_model <- round(cor(test.set$Number, predict(glm_model, test.set), method = c("pearson", "kendall", "spearman")),2)
corr_lasso_Mod <- round(cor(test.set$Number, predict(lasso_Mod, test.set), method = c("pearson", "kendall", "spearman")),2)
corr_knn_model <- round(cor(test.set$Number, predict(knn_model, test.set), method = c("pearson", "kendall", "spearman")),2)
corr_LF_model <- round(cor(test.set$Number, predict(LF_model, test.set), method = c("pearson", "kendall", "spearman")),2)
corr_LB_model <- round(cor(test.set$Number, predict(LB_model, test.set), method = c("pearson", "kendall", "spearman")),2)
corr_lmStepAIC_Mod <- round(cor(test.set$Number, predict(lmStepAIC_Mod, test.set), method = c("pearson", "kendall", "spearman")),2)

total_corr <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         corr_pred=c(corr_lm_model, corr_glm_model, corr_lasso_Mod, corr_knn_model, corr_LF_model, corr_LB_model, corr_lmStepAIC_Mod))
total_corr
```

## 27. RMSE between actual and predicted

```{r echo=TRUE, rows.print=30, cols.print=30}

#############################################################################################################################################
# Formula to calculate RMSE on test set
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

# RMSE value on test set
rmse_lm_model <- calc_rmse(actual = test.set$Number,
          predicted = predict(lm_model, test.set))

# RMSE value on test set
rmse_glm_model <- calc_rmse(actual = test.set$Number,
          predicted = predict(glm_model, test.set))

# RMSE value on test set
rmse_lasso_Mod <- calc_rmse(actual = test.set$Number,
          predicted = predict(lasso_Mod, test.set))

# RMSE value on test set
rmse_knn_model <- calc_rmse(actual = test.set$Number,
          predicted = predict(knn_model, test.set))

# RMSE value on test set
rmse_LF_model <- calc_rmse(actual = test.set$Number,
          predicted = predict(LF_model, test.set))

# RMSE value on test set
rmse_LB_model <- calc_rmse(actual = test.set$Number,
          predicted = predict(LB_model, test.set))

# RMSE value on test set
rmse_lmStepAIC_Mod <- calc_rmse(actual = test.set$Number,
          predicted = predict(lmStepAIC_Mod, test.set))

total_RMSE <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         RMSE_Pred=c(rmse_lm_model, rmse_glm_model, rmse_lasso_Mod, rmse_knn_model, rmse_LF_model, rmse_LB_model, rmse_lmStepAIC_Mod))
total_RMSE
```

## 28. MAE between actual and predicted

```{r echo=TRUE, rows.print=30, cols.print=30}

#############################################################################################################################################

MAE_test_lm_model <- MAE(test.set$Number, predict(lm_model, test.set))
MAE_test_glm_model <- MAE(test.set$Number, predict(glm_model, test.set))
MAE_test_lasso_Mod <- MAE(test.set$Number, predict(lasso_Mod, test.set))
MAE_test_knn_model <- MAE(test.set$Number, predict(knn_model, test.set))
MAE_test_LF_model <- MAE(test.set$Number, predict(LF_model, test.set))
MAE_test_LB_model <- MAE(test.set$Number, predict(LB_model, test.set))
MAE_test_lmStepAIC_Mod <- MAE(test.set$Number, predict(lmStepAIC_Mod, test.set))

total_MAE <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         MAE_Pred=c(MAE_test_lm_model, MAE_test_glm_model, MAE_test_lasso_Mod, MAE_test_knn_model, MAE_test_LF_model, MAE_test_LB_model, MAE_test_lmStepAIC_Mod))
total_MAE
```

## 29. r squared between actual and predicted

```{r echo=TRUE, rows.print=30, cols.print=30}

calc_rss = function(actual, predicted) {
  sum((predicted - actual) ^ 2)  ## residual sum of squares
}

calc_tss = function(actual, predicted) {
  sum((actual - mean(actual)) ^ 2)  ## total sum of squares
}

#######################################################
# rss calculation
calrss <- calc_rss(actual = test.set$Number,
                  predicted = predict(lm_model, test.set))

caltss <- calc_tss(actual = test.set$Number,
                  predicted = predict(lm_model, test.set))

rsq_lm_model <- 1 - calrss/caltss
#######################################################
# rss calculation
calrss <- calc_rss(actual = test.set$Number,
                  predicted = predict(glm_model, test.set))

caltss <- calc_tss(actual = test.set$Number,
                  predicted = predict(glm_model, test.set))

rsq_glm_model <- 1 - calrss/caltss
#######################################################
# rss calculation
calrss <- calc_rss(actual = test.set$Number,
                  predicted = predict(lasso_Mod, test.set))

caltss <- calc_tss(actual = test.set$Number,
                  predicted = predict(lasso_Mod, test.set))

rsq_lasso_Mod <- 1 - calrss/caltss
#######################################################
# rss calculation
calrss <- calc_rss(actual = test.set$Number,
                  predicted = predict(knn_model, test.set))

caltss <- calc_tss(actual = test.set$Number,
                  predicted = predict(knn_model, test.set))

rsq_knn_model <- 1 - calrss/caltss
#######################################################
# rss calculation
calrss <- calc_rss(actual = test.set$Number,
                  predicted = predict(LF_model, test.set))

caltss <- calc_tss(actual = test.set$Number,
                  predicted = predict(LF_model, test.set))

rsq_LF_model <- 1 - calrss/caltss
#######################################################
# rss calculation
calrss <- calc_rss(actual = test.set$Number,
                  predicted = predict(LB_model, test.set))

caltss <- calc_tss(actual = test.set$Number,
                  predicted = predict(LB_model, test.set))

rsq_LB_model <- 1 - calrss/caltss
#######################################################
# rss calculation
calrss <- calc_rss(actual = test.set$Number,
                  predicted = predict(lmStepAIC_Mod, test.set))

caltss <- calc_tss(actual = test.set$Number,
                  predicted = predict(lmStepAIC_Mod, test.set))

rsq_lmStepAIC_Mod <- 1 - calrss/caltss
#######################################################

total_rsq <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         R_squared_Pred=c(rsq_lm_model, rsq_glm_model, rsq_lasso_Mod, rsq_knn_model, rsq_LF_model, rsq_LB_model, rsq_lmStepAIC_Mod))
total_rsq



```

## 30. Combine predicted RMSE, MAE R squared, time

```{r echo=TRUE, rows.print=30, cols.print=30}

total_combALLpred <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         Model_TimePred,
                         total_RMSE,
                         total_MAE,
                         total_rsq) %>% 
                         dplyr::select(-2,-4,-6,-8)
total_combALLpred
```

## 31. Compare the two sets of RMSE, MAE, r squared, time

```{r echo=TRUE, rows.print=30, cols.print=30}

total_setscomp <- data.frame(Name=c("lm_model","glm_model","lasso_Mod", "knn_model","LF_model","LB_model","lmStepAIC_Mod"),
                         total_combALLpred,
                         total_best_train) %>% 
                         dplyr::select(-2,-7)
# Reorder columns
total_setscomp2 <- total_setscomp[,c(1,6,3,8,4,7,5,9,2)]

# Change name of column in data.frame
total_setscomp2 %>% 
  rename(
    RMSE_Train = RMSE,
    MAE_Train = MAE,
    Rsquared_Train = Rsquared
    )
```

## 30. RMSE, Rsquared MAE on training set

```{r echo=TRUE, rows.print=30, cols.print=30}
# RMSE, Rsquared MAE on training set
lm_model$results[c("RMSE","Rsquared","MAE")] %>%
        round(2)
# RMSE, Rsquared MAE on training set
glm_model$results[c("RMSE","Rsquared","MAE")] %>%
        round(2)
# RMSE, Rsquared MAE on training set
lasso_Mod$results[c("RMSE","Rsquared","MAE")] %>%
        round(2)
# RMSE, Rsquared MAE on training set
knn_model$results[c("RMSE","Rsquared","MAE")] %>%
        round(2)
# RMSE, Rsquared MAE on training set
LF_model$results[c("RMSE","Rsquared","MAE")] %>%
        round(2)
# RMSE, Rsquared MAE on training set
LB_model$results[c("RMSE","Rsquared","MAE")] %>%
        round(2)
# RMSE, Rsquared MAE on training set
lmStepAIC_Mod$results[c("RMSE","Rsquared","MAE")] %>%
        round(2)
```

## 31. 
## Model below is separate from models above
## Additional for testing: Decision Tree

```{r echo=TRUE}
summary(data)
class(data$Fire_Cause_Human) # [1] "factor"
unique(data$Fire_Cause_Human)
data$Fire_Cause_Human <- as.factor(data$Fire_Cause_Human) #Change to factor
class(data$Fire_Cause_Human)
unique(data$Fire_Cause_Human)
names(data)
str(data)

# Partition data into Training and Validation datasets
set.seed(123)
pd <- sample(2,nrow(data),replace = TRUE, prob = c(0.7,0.3))
train <-  data[pd==1,]
validate <-  data[pd==2,]

# Decision Tree with party
tree <- ctree(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + 
                Response.category, data = train, controls = ctree_control(mincriterion = 0.99, minsplit=900))

tree <- ctree(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train)
tree
plot(tree)
plot(tree, type="simple")

fire_ctree <- ctree(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = data)

print(fire_ctree)
plot(fire_ctree, type="simple")

train_index <- sample(1:nrow(data), 0.7 * nrow(data))
train.set <- data[train_index,]
test.set  <- data[-train_index,]

fire_ctree_model <- ctree(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data=train.set)
fire_ctree_model

fire_ctree_prediction <- predict(fire_ctree_model, test.set) 
# gives the probability for each class
head(fire_ctree_prediction)

table(fire_ctree_prediction, test.set$Number)
```

## 32. 
## Model below is separate from models above
## Additional for testing: Random Forest

```{r echo=TRUE}
# Check data before Random Forest
summary(data)
class(data$Fire_Cause_Human) # [1] "factor"
unique(data$Fire_Cause_Human)
data$Fire_Cause_Human <- as.factor(data$Fire_Cause_Human) #Change to factor
class(data$Fire_Cause_Human)
unique(data$Fire_Cause_Human)
names(data)
str(data)

train_index <- sample(1:nrow(data), 0.7 * nrow(data))
train.set <- data[train_index,]
test.set  <- data[-train_index,]

rf1 <- randomForest(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set)
rf1
print(rf1)
randomForest::importance(rf1) # Higher the value, the greater the importance.
plot(rf1) # Error levels of output 
table(train.set$Number)/nrow(train.set)
```

## 33. 
## Model below is separate from models above
## Additional for testing: Linear Regression for Predicting Forest Fires. 

```{r}
# Create total forest fires per year variable. 
require(dplyr)

print(unique(data[, 1]))  #Print column 1, "Cause"
    #   Cause
    # <fctr>
    # Forest industry				
    # Incendiary				
    # Lightning				
    # Miscellaneous known causes				
    # Other industry				
    # Railways				
    # Recreation				
    # Residents				
    # Unspecified				
    # Unspecified human activities

data$Fire_Cause_Human = as.character(data$Cause)
#character is easier to use than factor
#difference between factor and character

data$Fire_Cause_Human[ data$Fire_Cause_Human != "Lightning" & data$Fire_Cause_Human != "Unspecified" ] = "Human"
# There are three types of fire causes, Lightning, Human and Unspecified.  
# We don't know what unspecified is.  Unspecified could be Lightning or Human cause.

unique(data$Fire_Cause_Human)
#[1] "Human"       "Lightning"   "Unspecified"

names(data)
#  [1] "Cause"             "Jurisdiction"      "Number"            "Protection.zone"   "Response.category" "Year"              "Juris_Long"        "Cause_Grouped"     "Time1"             "Time2"            
# [11] "Region"            "Fire_Cause_Human" 

unique(data$Cause_Grouped)
#[1] "People"    "Lightning"

unique(data$Year)
#[1] 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018
############################
fm1 = Number ~ Juris_Long + Year + Fire_Cause_Human  + Response.category
#class(fm1) #[1] "formula"
#str(fm1)
  # Class 'formula'  language Number ~ Juris_Long + Year + Fire_Cause_Human + Response.category
  # ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 

model1 = lm(fm1, data = data)
#class(model1) #[1] "lm"
#str(model1) #Output is too long
summary(model1)

############################

model1 = lm(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = data)
#class(model1) #[1] "lm"
#all years

summary(model1)
# avg of the reference category for
#    (Alberta)
#    (cause is human)
#    (response category is Full)
#    (protection.zone is Intensive)
#Start with 331.0932 and add or subtract items to calculate specific query.

#Three ***, evidence there is an effect on y variable
# Unit of measure is number of fires
# predict y is number of fires
# Year variable has zero linear effect, no asterix *
# asterix *, Evaluating P value if there is a linear effect or not
#Estimate column is important
#Multiple R-squared:  0.1798
#R-squared: percentage of variation explained by the model. 18%
#Adjusted R-squared: takes into account how many variables in model total.
#p value for entire model: p-value: < 2.2e-16
#p value: we reject the NULL. All the coefficients are zero.
#p value comes from F-statistic

unique(data$Response.category)
# [1] Full        Modified    None        Unspecified
# Levels: Full Modified None Unspecified

unique(data$Protection.zone)
# [1] Intensive   Limited     Unspecified
# Levels: Intensive Limited Unspecified

```

## 34. 
## Model below is separate from models above
## Additional for testing: prediction

```{r echo=TRUE, rows.print=30, cols.print=30}

#Prediction for year 2018

#Line below is for 2018
#Train set
data2018 = filter(data, Year == 2018)

#Line below is everything except 2018
#Test set
datarm2018 = filter(data, Year != 2018)

#Linear model for all years except 2018?
modelrm2018 = lm(fm1, data = datarm2018)
#Summary on Test set
summary(modelrm2018)

#Predicting on (Train Set)
number_predict_2018 = predict(modelrm2018, newdata = dplyr::select(data2018, Juris_Long, Year, Fire_Cause_Human, Response.category), type = "response" )

print(number_predict_2018) #estimating y values, how many data points in new data.

length(number_predict_2018) #75 data points for 2018

length(data2018$Number) #75 data points in 2018

plot(data2018$Number, number_predict_2018) #comparing prediction with actual results
#plotting (actual y values of 2018) vs the (estimated y values of 2018)
#We are predicting less number of forest fires.
#Model only captures 18% of the variation.
#We are over estimating.

# The MSE of the model. 
mse_18 = mean((number_predict_2018  - data2018$Number)^2)
mse_18 #[1] 40489.22

# The RMSE of the model. ORIGINAL UNITS, NUMBER OF FIRES
# How many forest fires we are off by.
# We most likely overpredicted by 200 fires
rmse_18 = sqrt(mean((number_predict_2018  - data2018$Number)^2))
rmse_18 #[1] 201.2193

# The RMSE of the model. ORIGINAL UNITS, NUMBER OF FIRES
# How many forest fires we are off by.
# We most likely overpredicted by 200 fires
rmse_18_v2 = (mean((number_predict_2018  - data2018$Number)^2))^.5
rmse_18_v2 #[1] 201.2193

```

## 35. 
## RMSE Prediction for Years

```{r echo=TRUE, rows.print=30, cols.print=30}

# Function that calculates the rmse for the year specified. The rmse is calculated as the number of predicted fires minus the actual number of fires in that year and taking the squared mean of the differnce. 
rmse_year_function = function(data_set, year_predict, model_formula = Number ~ Juris_Long + Year + Fire_Cause_Human  + Response.category ) {
  
    data_test = filter(data_set, Year == year_predict)
    data_train = filter(data_set, Year < year_predict)
    
    model = lm( model_formula, data = data_train   )
    
    RMSE = sqrt(mean((predict(model, newdata = dplyr::select(data_test, Juris_Long, Year, Fire_Cause_Human, Response.category)) - data_test$Number)^2) )
  
    return(data.frame( Year = year_predict, RMSE = RMSE))
  
}
 
result18 = rmse_year_function(data_set = data, year_predict = 2018) 
result18 

result17 = rmse_year_function(data_set = data, year_predict = 2017) 
result17

result16 = rmse_year_function(data_set = data, year_predict = 2016) 
result16

final_result = rbind(result18, rmse_year_function(data_set = data, year_predict = 2017)  )
final_result

class(rmse_year_function(data_set = data, year_predict = 2018)) 

rmse_year_function(data_set = data, year_predict = 2017) 

rmse_year_function(data_set = data, year_predict = 2016) 


final_result = NULL
class(final_result)

for( i in 1991:2018){
  
  rmse_i = rmse_year_function(data_set = data, year_predict = i) 
  
  final_result = rbind(final_result, rmse_i)
  
}

dim(final_result)
class(final_result)

final_result


```

## 36. 
## Prepare Time Series Analysis.

```{r}

# install.packages("tseries")
require(forecast)  #for forecast function
require(tseries)

time_s = data %>% group_by(Year) %>% summarise(tot_fires = sum(Number))
plot(time_s) # Plot ALL, group by years

length(unique(data$Year)) #[1] 29

###################################################################################################################################
# Subset data for Alberta
time_s_alberta = subset(data, Juris_Long == "Alberta")
head(time_s_alberta)

# Group subsetted data for Alberta
plot_time_s_alberta = time_s_alberta %>% group_by(Year) %>% summarise(tot_fires = sum(Number))
plot(plot_time_s_alberta) # Plot Alberta
###################################################################################################################################
# Subset data for BC
time_s_BC = subset(data, Juris_Long == "British Columbia")
head(time_s_BC)

# Group subsetted data for Alberta
plot_time_s_BC = time_s_BC %>% group_by(Year) %>% summarise(tot_fires = sum(Number))
plot(plot_time_s_BC) # Plot Alberta
###################################################################################################################################

```

## 37. 
## Model: Time Series Analysis.

```{r}
# One liner below replaces the four lines below.
time_s$y = time_s$tot_fires
time_series_data = time_s
y = "Number of Total Fires"
y_time_series = time_series_data$y

y_time_series =  time_s$tot_fires
class(y_time_series) # [1] "integer"
#ts_total = ts(y_time_series, start = 1990, end = 2018, frequency = 1)
ts_total = ts(y_time_series, start = 1990, frequency = 1)
str(ts_total)
plot(ts_total)
###################################################################################################################################
# Alberta
y_time_series <- plot_time_s_alberta$tot_fires
class(y_time_series) # [1] "integer"
#ts_total = ts(y_time_series, start = 1990, end = 2018, frequency = 1)
plot_time_s_alberta <- ts(y_time_series, start = 1990, frequency = 1)
str(plot_time_s_alberta)
plot(plot_time_s_alberta)
###################################################################################################################################
# BC
y_time_series <- plot_time_s_BC$tot_fires
class(y_time_series) # [1] "integer"
#ts_total = ts(y_time_series, start = 1990, end = 2018, frequency = 1)
plot_time_s_BC <- ts(y_time_series, start = 1990, frequency = 1)
str(plot_time_s_BC)
plot(plot_time_s_BC)
```

## 38. 
## Model: Forecasting Time Series ARIMA Analysis.

```{r}
# Directly plotting a forecast of a model
plot(forecast(auto.arima(ts_total)))
# Forecast for Alberta
plot(forecast(auto.arima(plot_time_s_alberta)))
# Forecast for BC
plot(forecast(auto.arima(plot_time_s_BC)))
```


In time series analysis, an autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model. Both of these models are fitted to time series data either to better understand the data or to predict future points in the series (forecasting). ARIMA models are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the "integrated" part of the model) can be applied one or more times to eliminate the non-stationarity.[1]

The AR part of ARIMA indicates that the evolving variable of interest is regressed on its own lagged (i.e., prior) values. The MA part indicates that the regression error is actually a linear combination of error terms whose values occurred contemporaneously and at various times in the past. The I (for "integrated") indicates that the data values have been replaced with the difference between their values and the previous values (and this differencing process may have been performed more than once). The purpose of each of these features is to make the model fit the data as well as possible.

Non-seasonal ARIMA models are generally denoted ARIMA(p,d,q) where parameters p, d, and q are non-negative integers, p is the order (number of time lags) of the autoregressive model, d is the degree of differencing (the number of times the data have had past values subtracted), and q is the order of the moving-average model. Seasonal ARIMA models are usually denoted ARIMA(p,d,q)(P,D,Q)m, where m refers to the number of periods in each season, and the uppercase P,D,Q refer to the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model.[2][3]

When two out of the three terms are zeros, the model may be referred to based on the non-zero parameter, dropping "AR", "I" or "MA" from the acronym describing the model. For example, ARIMA (1,0,0) is AR(1), ARIMA(0,1,0) is I(1), and ARIMA(0,0,1) is MA(1).

```{r}

```

## 39. 
## Analysis of Stationarity
## Autocorrelation function (ACF) of the time series

```{r}
#checking for asumptions of model.
acf(time_series_data$y, main = paste("ACF of", y))  #Autocorrelation (lags outside 95% confidence bands) implies stochastic or deterministic trend
```

## 40. 
## Plot of the differenced time series

```{r}

diff_y_time_series = diff(log(y_time_series))
plot(diff_y_time_series, type = "line", main = paste('Differenced ', y))  #Visual proof of weakly stationary mean and variance 
```

## 41. 
## Autocorrelation function (ACF) of the differenced time series

```{r}

acf(na.omit(diff_y_time_series), main = paste('ACF of Differenced', y))  #Lags within the 95% confidence bands are considered noise and are statistically insignificant (trend removed). Lags outside 95% confidence bands determine the order of the MA(q) process -- an MA process of the order q has an ACF that cuts off after q lags.
```

## 42. 
## Partial autocorrelation function (PACF) of the differenced time series

```{r}

pacf(na.omit(diff_y_time_series), main = paste('PACF of Differenced', y))  #Lags within the 95% confidence bands are considered noise and are statistically insignificant (trend removed). Lags outside 95% confidence bands determine the order of the AR(p) process -- an AR process of the order p has a PACF that cuts off after p lags.
```

## 43. 
## Ljung-Box test of autocorrelation

```{r}
Box.test(diff_y_time_series, lag = log(length(diff_y_time_series)), type = "Ljung-Box")  #Formal test of autocorrelation and/or associated trend. The null hypothesis is that there is no autocorrelation (confirmed by p-value > 5%)
```

## 44. 
## Augmented Dickey-Fuller test of stationarity

```{r}
library(tseries)
adf.test(na.omit(diff_y_time_series))  #Formal test of stationarity. The null hypothesis is that the process is NOT stationary.  We look for a p-value < 5% to reject the null hypothesis in favor of the alternative hypothesis (in which case the process is stationary)
```

## 40. 
## Additional testing
## Variable Selection for Multiple Linear Regression in R

```{r echo=TRUE}
# Split the dataset to 70% of training and 30% of test sets. 
# We want to make sure that the training set and the test set do not have any common data points.
set.seed(123)

train_index <- sample(1:nrow(data), 0.7 * nrow(data))
train.set <- data[train_index,]
test.set  <- data[-train_index,]

# Train our model on the training set
model2 = lm(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = train.set)
#summary(model2)

# Prediction on the test set
prediction <- predict(model2, interval="prediction", newdata =test.set)
#model2

# Calculate error (prediction Number - test Number) in predictions and show the histogram of error
errors <- prediction[,"fit"] - test.set$Number
hist(errors)

# Compute the root mean square error and find the percentage of cases with less than 25% error.
# Calculate mean square error (mse) and find the percentage of cases with less than 25% error.

rmse <- sqrt(sum((prediction[,"fit"] - test.set$Number)^2)/nrow(test.set))
rel_change <- 1 - ((test.set$Number - abs(errors)) / test.set$Number)

pred25 <- table(rel_change<0.25)["TRUE"] / nrow(test.set)
paste("RMSE:", rmse)
paste("PRED(25):", pred25)

# Use simple linear regression model by using 'Juris_Long' as an independent variable. Compare the results with the multiple linear regression.

rn_train <- sample(nrow(data), floor(nrow(data)*0.7))
train <- data[rn_train,c("Number","Juris_Long")]
test <- data[-rn_train,c("Number","Juris_Long")]
model_ulm <- lm(Number~Juris_Long, data=train)
prediction <- predict(model_ulm, interval="prediction", newdata =test)
errors <- prediction[,"fit"] - test$Number
hist(errors)

rmse <- sqrt(sum((prediction[,"fit"] - test$Number)^2)/nrow(test))
rel_change <- 1 - ((test$Number - abs(errors)) / test$Number)
pred25 <- table(rel_change<0.25)["TRUE"] / nrow(test)
paste("RMSE:", rmse)
paste("PRED(25):", pred25)

# Both Pred(25) and RMSE values are better for multiple linear regression.

```

## 41. 
## Additional testing
## Forward and Backward selection algorithm 

```{r echo=TRUE, rows.print=30, cols.print=30}

library(MASS) # stepwise regression
#install.packages('leaps')
library(leaps) # all subsets regression

#install.packages('FNN')
library(FNN)

# Forward selection 
full <- lm(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = data)
null <- lm(Number~1,data=data)
stepF <- stepAIC(null, scope=list(lower=null, upper=full), direction= "forward", trace=TRUE)
summary(stepF)
# We end up using all the variables. We set 'trace=TRUE' to see all the steps.

# We can also use 'backward' elimination, which will start with 'full'.
full <- lm(Number ~ Juris_Long + Year + Fire_Cause_Human + Protection.zone + Response.category, data = data)
stepB <- stepAIC(full, direction= "backward", trace=TRUE)
summary(stepB)
# We end up using all the variables.

```

## 42. 
## Additional testing
## Variable selection using automatic methods

```{r echo=TRUE}

# Best combination of the 6 attributes.
subsets<-regsubsets(Number ~ Jurisdiction + Year + Fire_Cause_Human + Protection.zone + Response.category, data = data, nbest=1)
sub.sum <- summary(subsets)
as.data.frame(sub.sum$outmat)

# In the output * denotes the included variables. 
# The best combination of 4 attributes is: 'Fire_Cause_HumanLightning', 'JurisdictionBC', 'Response.categoryModified' and 'Response.categoryNone'. 
# The best combination of 5 attributes is: 'Fire_Cause_HumanLightning', 'JurisdictionBC', 'Response.categoryModified', 'Response.categoryNone' and Protection.zoneLimited
```

## 43. 
## Additional testing 
# Prediction using k Nearest Neighbor Regression

```{r echo=TRUE}

unique(data$Fire_Cause_Human) # [1] Human       Lightning   Unspecified
unique(data$Juris_Long) # [1] Alberta                   British Columbia   ...
unique(data$Protection.zone) # [1] Intensive   Limited     Unspecified
unique(data$Response.category) # [1] Full        Modified    None        Unspecified
unique(data$Year) # 1990 1991 1992 1993 1994 1995 1996 

# rm(data1)
# rm(dataset.numeric)
data1 <- data[, c("Number","Fire_Cause_Human", "Juris_Long", "Protection.zone", "Response.category", "Year")]
head(data1)
tail(data1)
str(data1)
summary(data1)
# new_fire <- fires[, c("Cause", "Jurisdiction", "Number", "Protection.zone", "Response.category", "Year")]

# sum(is.na(data1$Number))
# sum(is.na(data1$Fire_Cause_Human))
# sum(is.na(data1$Juris_Long))
# sum(is.na(data1$Protection.zone))
# sum(is.na(data1$Response.category))
# sum(is.na(data1$Year))


dataset <- rbind(data1, c(0,'Lightning','Alberta','Intensive','Full','1996'))

str(dataset)
unique(dataset$Number) # [1] Human       Lightning   Unspecified
unique(dataset$Fire_Cause_Human) # [1] Human       Lightning   Unspecified
unique(dataset$Juris_Long) # [1] Alberta                   British Columbia   ...
unique(dataset$Protection.zone) # [1] Intensive   Limited     Unspecified
unique(dataset$Response.category) # [1] Full        Modified    None        Unspecified
unique(dataset$Year) # 1990 1991 1992 1993 1994 1995 1996 

dataset[is.na(dataset)] <- 0
dataset.numeric <- sapply( dataset[,1:6], as.numeric )
#Should convert data to numeric to use knn.reg

#Remove NAs from matrix, replace with zero
dataset.numeric[is.na(dataset.numeric)] <- 0
#Matrix now has NAs removed, now replaced with zero

#unique(dataset.numeric$Fire_Cause_Human) 

dataset.numeric <- as.data.frame(dataset.numeric)
prediction <- knn.reg(dataset.numeric[1:nrow(data1),-1],
test = dataset.numeric[nrow(data1)+1,-1],
    dataset.numeric[1:nrow(data1),]$Number, k = 7 , algorithm="kd_tree")

prediction$pred # [1] 498.4286
```

## Additional time series data

```{r echo=TRUE, rows.print=30, cols.print=30}

ts_forecast_cov = function(data, covariate , subset = "Province" ) {
  # Subset = "Cause" or "Province"
  # Covariate = "Human", "Lightning","Unspecified" or one of the Provinces.

  if (subset == "Cause"){
    time_s = subset(data,Fire_Cause_Human == covariate)  %>% group_by(Year) %>% summarise(tot_fires = sum(Number))
  } else {
    time_s = subset(data, Juris_Long == covariate)  %>% group_by(Year) %>% summarise(tot_fires = sum(Number))
  }
  y_time_series = time_s$tot_fires
  ts_total = ts(y_time_series, start = 1990, frequency = 1)
  plot(forecast(auto.arima(ts_total)))
}
#y = "Number of Total Fires"
# Different subsets and covariates of the data.
ts_forecast_cov(data, covariate = "Alberta", subset = "Province" )
ts_forecast_cov(data, covariate = "Ontario", subset = "Province" )
ts_forecast_cov(data, covariate = "Quebec", subset = "Province" )
ts_forecast_cov(data, covariate = "British Columbia", subset = "Province" )
ts_forecast_cov(data, covariate = "Human", subset = "Cause" )
ts_forecast_cov(data, covariate = "Lightning", subset = "Cause" )
```

```


Notes for myself


RMSE

The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data-how close the observed data points are to the model's predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction.

The F-test

The F-test evaluates the null hypothesis that all regression coefficients are equal to zero versus the alternative that at least one is not. An equivalent null hypothesis is that R-squared equals zero. A significant F-test indicates that the observed R-squared is reliable and is not a spurious result of oddities in the data set. Thus the F-test determines whether the proposed relationship between the response variable and the set of predictors is statistically reliable and can be useful when the research objective is either prediction or explanation.

R-squared and Adjusted R-squared

The difference between SST and SSE is the improvement in prediction from the regression model, compared to the mean model. Dividing that difference by SST gives R-squared. It is the proportional improvement in prediction from the regression model, compared to the mean model. It indicates the goodness of fit of the model.

R-squared has the useful property that its scale is intuitive: it ranges from zero to one, with zero indicating that the proposed model does not improve prediction over the mean model, and one indicating perfect prediction. Improvement in the regression model results in proportional increases in R-squared.

One pitfall of R-squared is that it can only increase as predictors are added to the regression model. This increase is artificial when predictors are not actually improving the model's fit. To remedy this, a related statistic, Adjusted R-squared, incorporates the model's degrees of freedom. Adjusted R-squared will decrease as predictors are added if the increase in model fit does not make up for the loss of degrees of freedom. Likewise, it will increase as predictors are added if the increase in model fit is worthwhile. Adjusted R-squared should always be used with models with more than one predictor variable. It is interpreted as the proportion of total variance that is explained by the model.

There are situations in which a high R-squared is not necessary or relevant. When the interest is in the relationship between variables, not in prediction, the R-square is less important. An example is a study on how religiosity affects health outcomes. A good result is a reliable relationship between religiosity and health. No one would expect that religion explains a high percentage of the variation in health, as health is affected by many other factors. Even if the model accounts for other variables known to affect health, such as income and age, an R-squared in the range of 0.10 to 0.15 is reasonable.

```{r echo=TRUE}


```


